{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### doc2vec\n",
    "\n",
    "- Two Layer **Neural Network** that accepts a Text Corpus as an Input\n",
    "\n",
    "- Returns Set of **Vectors** ( Embeddings )\n",
    "\n",
    "- One **Numeric** Vector for each **Sentence**    \n",
    "\n",
    "- doc2vec Vectors are converted to **List** unlike **Array** in word2vec\n",
    "\n",
    "e.g. My Name is Kirankumar (It will be Passed to a Two Layer Neural Network)\n",
    "\n",
    "The Neural Network will Return a Numeric Vector for entire Sentence ( My Name is Kirankumar )\n",
    "\n",
    "\n",
    "**Train** Our Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                    Text  \n",
       "0                                        Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n",
       "1                                                                                                                          Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007...  \n",
       "3                                                                                                      U dun say so early hor... U c already then say...  \n",
       "4                                                                                          Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth',150)\n",
    "\n",
    "msg = pd.read_csv('../Data/Spam.csv', encoding='latin-1')\n",
    "msg.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], axis=1, inplace=True)\n",
    "msg.rename(columns={'v1':'Label', 'v2':'Text'}, inplace=True)\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Data using Built in **Cleaner** in Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Text</th>\n",
       "      <th>Clean Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, there, got, amore, wat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 0845281007...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, apply, over]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                                                                    Text  \\\n",
       "0                                        Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...   \n",
       "1                                                                                                                          Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 0845281007...   \n",
       "3                                                                                                      U dun say so early hor... U c already then say...   \n",
       "4                                                                                          Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                                                                       Clean Text  \n",
       "0                           [go, until, jurong, point, crazy, available, only, in, bugis, great, world, la, buffet, cine, there, got, amore, wat]  \n",
       "1                                                                                                                     [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive, entry, question, std, txt, rate, apply, over]  \n",
       "3                                                                                                  [dun, say, so, early, hor, already, then, say]  \n",
       "4                                                                           [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg['Clean Text'] = msg['Text'].apply(lambda x : gensim.utils.simple_preprocess(x))\n",
    "msg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the Dataset into **Train** Set and **Test** Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(msg['Clean Text'], \n",
    "                                                    msg['Label'],\n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Tagged Document Object to Train the **doc2vec** Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['no', 'in', 'the', 'same', 'boat', 'still', 'here', 'at', 'my', 'moms', 'check', 'me', 'out', 'on', 'yo', 'half', 'naked'], tags=[0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_doc = [gensim.models.doc2vec.TaggedDocument(vector, [i]) for i, vector in enumerate(X_train)]\n",
    "\n",
    "tagged_doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = gensim.models.Doc2Vec(tagged_doc, \n",
    "                            vector_size=100, \n",
    "                            windows=5, \n",
    "                            min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass a List of Words to a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.82968388e-03,  4.13799053e-03, -6.63741725e-04,  2.40805373e-03,\n",
       "        7.69969629e-05,  7.21669756e-03,  6.78393478e-03,  1.06860707e-02,\n",
       "        1.09892734e-03, -7.62522686e-03, -2.73938896e-03,  1.57334656e-02,\n",
       "        6.02932228e-03, -3.71735357e-03, -1.19462544e-02, -3.49768251e-03,\n",
       "        1.73992815e-03, -2.83530238e-03, -1.82010245e-03,  1.06529947e-02,\n",
       "       -3.43838055e-03,  6.64628018e-03, -1.19453052e-03,  2.39211321e-03,\n",
       "       -5.88744367e-03, -1.15917539e-02,  2.04924308e-03, -1.85354752e-03,\n",
       "        1.45837956e-03, -8.04794487e-03,  1.46116065e-02, -8.18478409e-04,\n",
       "       -1.07960682e-02, -8.38901382e-03,  1.96016319e-02, -4.47415514e-03,\n",
       "       -2.09215493e-03, -9.65477549e-04,  6.97840098e-03, -2.13088561e-02,\n",
       "        8.44003749e-04,  1.72075015e-02,  5.17110620e-03,  7.43819447e-03,\n",
       "        5.57388738e-03,  1.15229702e-02,  5.20422123e-03, -1.65762659e-02,\n",
       "        1.66290104e-02,  3.58728948e-03,  1.84620603e-03, -4.49036481e-03,\n",
       "        2.92143726e-04, -1.50679459e-03, -1.14430292e-02,  3.65502574e-03,\n",
       "        9.33048781e-03,  2.34295055e-03, -6.22204598e-03,  6.49673492e-03,\n",
       "       -4.80215997e-03, -1.07688399e-03,  4.75783646e-03, -1.02645755e-02,\n",
       "       -1.12444896e-03,  2.48791883e-03,  1.20239751e-03,  8.57360754e-03,\n",
       "       -2.68695992e-04,  4.55627684e-03, -1.73051022e-02, -1.75453946e-02,\n",
       "       -1.56054355e-03,  7.19744130e-04,  2.50457641e-04,  1.80409523e-04,\n",
       "       -1.57555453e-02,  5.46735665e-03,  4.61287471e-03, -6.75823493e-03,\n",
       "        4.09051869e-03, -2.20453925e-02,  7.55185913e-03,  2.67480756e-03,\n",
       "        3.32862302e-03,  2.77718809e-05,  1.64935854e-03, -5.66705130e-03,\n",
       "       -4.60606907e-03, -4.58370103e-03,  2.84486939e-03,  4.27464722e-04,\n",
       "        9.96787287e-03, -3.74967093e-03,  1.15467957e-03, -5.98736515e-04,\n",
       "        4.50948812e-03, -1.06264083e-02, -4.49524610e-04,  5.89403044e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v.infer_vector(['i','am','learning','nlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not so many **Pretrained** Document Vectors and **API's** like Word Vectors\n",
    "\n",
    "Prepare the Vectors to be used in a **Machine Learning Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.00739162,  0.04333074,  0.02113032, -0.00489163,  0.00191754,\n",
       "         0.02435322,  0.00541513,  0.02438186, -0.00044579, -0.01962186,\n",
       "        -0.01884066,  0.06329914,  0.05002325, -0.03069448, -0.03426618,\n",
       "        -0.01110566, -0.0113846 ,  0.00360292, -0.01247877,  0.04154537,\n",
       "        -0.02077973,  0.02628371, -0.02138242,  0.01969699, -0.00769653,\n",
       "        -0.04228035,  0.00259427, -0.02075599, -0.00118174, -0.02084192,\n",
       "         0.05354832, -0.00400604, -0.0310108 , -0.02013468,  0.0898321 ,\n",
       "        -0.03106854,  0.00561934, -0.0274569 ,  0.04312494, -0.0816195 ,\n",
       "         0.00027242,  0.07861911,  0.01129802,  0.03486763,  0.042072  ,\n",
       "         0.03331375,  0.03659694, -0.05415655,  0.07201695,  0.00617635,\n",
       "        -0.0055012 , -0.01186855, -0.00340939, -0.02485179, -0.04848693,\n",
       "         0.0209582 ,  0.03250092,  0.00464377, -0.04998276,  0.02563726,\n",
       "        -0.04112181, -0.00269202,  0.00824046, -0.06723288, -0.00888634,\n",
       "         0.01006795, -0.00038465,  0.04945185,  0.02044252,  0.00011828,\n",
       "        -0.10123821, -0.07514659,  0.01215603,  0.03060658, -0.00497354,\n",
       "        -0.00840085, -0.05532077,  0.00703672,  0.01150182, -0.01166408,\n",
       "        -0.00377795, -0.08203332,  0.01655698, -0.00209899,  0.03699374,\n",
       "         0.00449521, -0.00185896, -0.0247644 , -0.0095273 , -0.0478579 ,\n",
       "         0.03177303, -0.01600815,  0.05198211, -0.01398389,  0.01945699,\n",
       "        -0.01887525,  0.05092643, -0.05291465,  0.00301753,  0.04867708],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [[d2v.infer_vector(word)] for word in X_test]\n",
    "vectors[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
